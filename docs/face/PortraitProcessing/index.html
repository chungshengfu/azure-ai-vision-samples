<!DOCTYPE html>
<meta charset="utf-8">
<title>Portrait Processing</title>
<style>
table, th, td{
  border: 1px solid;
}
</style>
<script>
$_GET = location.search.substr(1).split('&').reduce((o, i) => {
  const [k, v] = i.split('=');
  if (k) {
    o[k] = v;
  }
  return o;
}, {});

document.addEventListener('DOMContentLoaded', async () => {
  const MAX_IMAGE_SIZE = 800;
  const JPEG_QUALITY = 0.95;
  const TOP_MARGIN_MAX = 0.75;
  const BOTTOM_MARGIN_MAX = 0.75;
  const LEFT_MARGIN_MAX = 1.5;
  const RIGHT_MARGIN_MAX = 1.5;

  const onImageSelected = async (e) => {
    const [file] = e.target.files;
    if (!file) {
      return;
    }

    reset();

    const inputImage = resizeImage(await createImageBitmap(file));
    document.body.appendChild(document.createElement('p')).appendChild(inputImage);

    const faceDetectionResults = await detectFaces(await compressImage(inputImage));
    const table = document.createElement('table');
    const thead = table.appendChild(document.createElement('thead'));
    thead.appendChild(['Cropped Face', '(Intermediate) Output Matting', 'Output Portrait'].reduce((tr, header) => {
      const th = tr.appendChild(document.createElement('th'));
      th.textContent = header;
      return tr;
    }, thead.appendChild(document.createElement('tr'))));
    const tbody = table.appendChild(document.createElement('tbody'));
    const promises = faceDetectionResults.map(faceDetectionResult => processPortrait(inputImage, tbody, faceDetectionResult));
    await Promise.all(promises);
    document.body.appendChild(table);
  };

  const reset = () => {
    const p = document.getElementById('input')?.parentElement;
    while (p?.nextElementSibling) {
      p.nextElementSibling.remove();
    }
  };

  const drawImage = (image, scale = 1) => {
    const { width, height } = image;
    const canvas = document.createElement('canvas');
    const ctx = canvas.getContext('2d');
    canvas.width = width * scale;
    canvas.height = height * scale;
    ctx.scale(scale, scale);
    ctx.drawImage(image, 0, 0, width, height);
    return canvas;
  };

  const resizeImage = (image) => {
    const scale = Math.min(MAX_IMAGE_SIZE / image.width, MAX_IMAGE_SIZE / image.height, 1);
    return drawImage(image, scale);
  };

  const compressImage = (canvas) => new Promise((resolve) => {
    canvas.toBlob(resolve, 'image/jpeg', JPEG_QUALITY);
  });

  const detectFaces = async (blob) => {
    const response = await fetch(`https://${$_GET['face-endpoint']}.cognitiveservices.azure.com/face/v1.1-preview.1/detect?detectionModel=detection_03&recognitionModel=recognition_04&returnFaceId=false&returnFaceAttributes=blur,headPose,mask,qualityForRecognition&returnFaceLandmarks=true`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/octet-stream',
        'Ocp-Apim-Subscription-Key': $_GET['face-apikey'],
      },
      body: blob,
    });
    return await response.json();
  };

  const cropFace = (canvas, faceRectangle) => {
      const { left, top, width, height } = faceRectangle;
      const cropLeft = Math.max(Math.floor(left - width * LEFT_MARGIN_MAX), 0);
      const cropTop = Math.max(Math.floor(top - height * TOP_MARGIN_MAX), 0);
      const cropRight = Math.min(Math.floor(left + width * (1 + RIGHT_MARGIN_MAX)), canvas.width);
      const cropBottom = Math.min(Math.floor(top + height * (1 + BOTTOM_MARGIN_MAX)), canvas.height);
      const cropWidth = cropRight - cropLeft;
      const cropHeight = cropBottom - cropTop;
      const croppedFace = document.createElement('canvas');
      croppedFace.width = cropWidth;
      croppedFace.height = cropHeight;
      croppedFace.getContext('2d').drawImage(canvas, cropLeft, cropTop, cropWidth, cropHeight, 0, 0, cropWidth, cropHeight);
      return croppedFace;
  };

  const removeBackground = async (blob) => {
    const response = await fetch(`https://${$_GET['vision-endpoint']}.cognitiveservices.azure.com/computervision/imageanalysis:segment?api-version=2023-04-01-preview&mode=foregroundMatting&overload=stream`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/octet-stream',
        'Ocp-Apim-Subscription-Key': $_GET['vision-apikey'],
      },
      body: blob,
    });
    // if (response.status !== 200) {
    //   const error = await response.json();
    //   debugger;
    //   return error;
    // }
    return await response.blob();
  };

  const processPortrait = async (inputImage, resultTable, faceDetectionResult) => {
      const tr = document.createElement('tr');
      const faceImage = cropFace(inputImage, faceDetectionResult.faceRectangle);
      const foregroundImage = drawImage(await createImageBitmap(await removeBackground(await compressImage(faceImage))));
      const resultImage = drawImage(faceImage);
      const ctx = resultImage.getContext('2d');
      ctx.globalCompositeOperation = 'multiply';
      ctx.drawImage(foregroundImage, 0, 0, resultImage.width, resultImage.height);
      resultTable.appendChild([faceImage, foregroundImage, resultImage].reduce((tr, image) => {
        tr.appendChild(document.createElement('td')).appendChild(image);
        return tr;
      }, tr));
    };

  document.getElementById('input').addEventListener('change', onImageSelected);
});
</script>
<p>
  <a href="https://github.com/chungshengfu/azure-ai-vision-samples/tree/dev/face/PortraitProcessing">Source code</a>
</p>
<p>
  <input type="file" id="input" accept="image/*" />
</p>